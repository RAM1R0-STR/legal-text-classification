{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n\nhtml = \"\"\"\n<div style=\"background:linear-gradient(135deg,#1a1a2e,#16213e,#0f3460);border-radius:16px;padding:40px;text-align:center;font-family:'Segoe UI',Arial,sans-serif;box-shadow:0 8px 32px rgba(0,0,0,0.4);\">\n  <div style=\"font-size:2.5em;\">ðŸ¤–</div>\n  <h1 style=\"color:#e0e0e0;font-size:2em;margin:8px 0;\">Modelado y Evaluacion</h1>\n  <p style=\"color:#a0aec0;letter-spacing:2px;text-transform:uppercase;font-size:0.95em;\">Clasificacion Â· Balanceo Â· GridSearchCV Â· Evaluacion</p>\n  <div style=\"background:rgba(255,255,255,0.06);border:1px solid rgba(255,255,255,0.1);border-radius:10px;padding:16px 24px;max-width:650px;margin:24px auto 0;text-align:left;color:#cbd5e0;font-size:0.93em;line-height:1.7;\">\n    Entrenamos y comparamos multiples modelos de clasificacion sobre los textos legales vectorizados.\n    Manejamos el desbalance extremo de clases con agrupacion, Random Oversampling y SMOTE.\n    Optimizamos el mejor modelo con GridSearchCV.\n  </div>\n</div>\n\"\"\"\ndisplay(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Artefactos\n\nCargamos los artefactos generados por `02_preprocesamiento.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\nimport pandas as pd\nimport numpy as np\n\nX = joblib.load(\"../models/X_tfidf.pkl\")\ny = joblib.load(\"../models/y_labels.pkl\")\ndf = pd.read_csv(\"../data/raw/df_limpio.csv\")\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"Clases: {y.nunique()}\")\nprint(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"Train: {X_train.shape[0]:,}  |  Test: {X_test.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparacion de Modelos\n\nComparamos cuatro algoritmos usando F1 Macro como metrica principal, ya que penaliza por igual el mal desempeno en clases minoritarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, f1_score\nimport matplotlib.pyplot as plt\n\nmodelos = {\n    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=1.0),\n    \"Naive Bayes\":         MultinomialNB(alpha=0.1),\n    \"LinearSVC\":           LinearSVC(class_weight=\"balanced\", max_iter=2000, C=1.0),\n    \"Random Forest\":       RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n}\n\nresultados = {}\nfor nombre, modelo in modelos.items():\n    modelo.fit(X_train, y_train)\n    y_pred = modelo.predict(X_test)\n    acc      = np.mean(y_pred == y_test)\n    f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n    f1_w     = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n    resultados[nombre] = {\"Accuracy\": acc, \"F1 Macro\": f1_macro, \"F1 Weighted\": f1_w}\n    print(f\"\\n{'='*40}\\n  {nombre}\\n  Accuracy: {acc:.4f}  F1 Macro: {f1_macro:.4f}\")\n\ndf_res = pd.DataFrame(resultados).T.sort_values(\"F1 Macro\", ascending=False)\nprint(\"\\n=== Ranking ===\")\nprint(df_res.round(4))\n\n# Grafico\nfig, ax = plt.subplots(figsize=(9, 5))\ndf_res_s = df_res.sort_values(\"F1 Macro\")\nax.barh(df_res_s.index, df_res_s[\"F1 Macro\"], color=\"#4299e1\", alpha=0.85)\nax.set_xlabel(\"F1 Macro\")\nax.set_title(\"Comparacion de Modelos - F1 Macro\")\nax.xaxis.grid(True, alpha=0.4)\nfor i, v in enumerate(df_res_s[\"F1 Macro\"]):\n    ax.text(v + 0.003, i, f\"{v:.4f}\", va=\"center\", fontsize=9)\nplt.tight_layout()\nplt.savefig(\"../reports/model_comparison.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizacion con GridSearchCV\n\nBuscamos el mejor hiperparametro C para LinearSVC usando validacion cruzada de 5 pliegues optimizando F1 Macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n\ngrid_search = GridSearchCV(\n    LinearSVC(class_weight=\"balanced\", max_iter=3000),\n    {\"C\": [0.01, 0.1, 1.0, 10.0]},\n    cv=5, scoring=\"f1_macro\", n_jobs=-1, verbose=1\n)\ngrid_search.fit(X_train, y_train)\n\nmejor_modelo = grid_search.best_estimator_\ny_pred_best  = mejor_modelo.predict(X_test)\n\nprint(f\"Mejor C: {grid_search.best_params_}\")\nprint(f\"F1 Macro CV: {grid_search.best_score_:.4f}\")\nprint(\"\\n=== LinearSVC Optimizado ===\")\nprint(classification_report(y_test, y_pred_best, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manejo de Desbalance\n\n**Paso 1:** Agrupamos las 3 clases con menos de 200 ejemplos en \"otros\" (de 10 a 8 clases).\n**Paso 2:** Aplicamos oversampling SOLO sobre train para balancear la representacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n\n# Agrupacion\nclases_min = [\"affirmed\", \"approved\", \"related\"]\ndf[\"case_outcome_agrupado\"] = df[\"case_outcome\"].apply(\n    lambda x: \"otros\" if x in clases_min else x\n)\n\ndist_orig = df[\"case_outcome\"].value_counts()\ndist_agr  = df[\"case_outcome_agrupado\"].value_counts()\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\naxes[0].barh(dist_orig.index, dist_orig.values, color=\"#5bc0de\", alpha=0.85)\naxes[0].set_title(\"Original (10 clases)\")\nfor i, v in enumerate(dist_orig.values): axes[0].text(v+30, i, str(v), va=\"center\", fontsize=9)\n\naxes[1].barh(dist_agr.index, dist_agr.values, color=\"#5cb85c\", alpha=0.85)\naxes[1].set_title(\"Agrupado (8 clases)\")\nfor i, v in enumerate(dist_agr.values): axes[1].text(v+30, i, str(v), va=\"center\", fontsize=9)\n\nplt.suptitle(\"Agrupamiento de Clases Minoritarias\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"../reports/distribucion_clases.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\nprint(f\"Ratio max/min antes:  {dist_orig.max()//dist_orig.min()}x\")\nprint(f\"Ratio max/min despues: {dist_agr.max()//dist_agr.min()}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balanceo con Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import accuracy_score\n\nvectorizer_ag = TfidfVectorizer(max_features=10000, ngram_range=(1,2), min_df=3, max_df=0.85, sublinear_tf=True)\nX_ag = vectorizer_ag.fit_transform(df[\"clean_text\"])\ny_ag = df[\"case_outcome_agrupado\"]\n\nX_train_ag, X_test_ag, y_train_ag, y_test_ag = train_test_split(\n    X_ag, y_ag, test_size=0.2, random_state=42, stratify=y_ag\n)\n\nros = RandomOverSampler(random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train_ag, y_train_ag)\n\nprint(f\"Train antes:  {X_train_ag.shape[0]:,} | despues: {X_train_ros.shape[0]:,}\")\nprint(\"Distribucion despues del oversampling:\")\nprint(pd.Series(y_train_ros).value_counts())\n\nmodelo_ros = LinearSVC(max_iter=3000, C=1.0)\nmodelo_ros.fit(X_train_ros, y_train_ros)\ny_pred_ros = modelo_ros.predict(X_test_ag)\n\nf1_ros  = f1_score(y_test_ag, y_pred_ros, average=\"macro\", zero_division=0)\nacc_ros = accuracy_score(y_test_ag, y_pred_ros)\nprint(f\"\\n=== Agrupacion + Random Oversampling ===\")\nprint(classification_report(y_test_ag, y_pred_ros, zero_division=0))\nprint(f\"F1 Macro: {f1_ros:.4f}  |  Accuracy: {acc_ros:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Balanceo con SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n\nprint(\"Aplicando SMOTE...\")\nX_train_dense = X_train_ag.toarray()\nX_test_dense  = X_test_ag.toarray()\n\nsmote = SMOTE(random_state=42, k_neighbors=3)\nX_train_sm, y_train_sm = smote.fit_resample(X_train_dense, y_train_ag)\n\nprint(f\"Train antes: {X_train_dense.shape[0]:,} | despues: {X_train_sm.shape[0]:,}\")\n\nmodelo_smote = LinearSVC(max_iter=3000, C=1.0)\nmodelo_smote.fit(X_train_sm, y_train_sm)\ny_pred_smote = modelo_smote.predict(X_test_dense)\n\nf1_smote  = f1_score(y_test_ag, y_pred_smote, average=\"macro\", zero_division=0)\nacc_smote = accuracy_score(y_test_ag, y_pred_smote)\nprint(f\"\\n=== Agrupacion + SMOTE ===\")\nprint(classification_report(y_test_ag, y_pred_smote, zero_division=0))\nprint(f\"F1 Macro: {f1_smote:.4f}  |  Accuracy: {acc_smote:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparacion Final de Enfoques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_baseline = f1_score(y_test, y_pred_best, average=\"macro\", zero_division=0)\nacc_baseline = accuracy_score(y_test, y_pred_best)\n\nenfoques    = [\"Baseline\\n(10 cl.)\", \"Agrup. +\\nRandom OS\", \"Agrup. +\\nSMOTE\"]\nf1_vals     = [f1_baseline, f1_ros, f1_smote]\nacc_vals    = [acc_baseline, acc_ros, acc_smote]\n\nx = np.arange(len(enfoques))\nwidth = 0.35\nfig, ax = plt.subplots(figsize=(10, 6))\nb1 = ax.bar(x - width/2, f1_vals,  width, label=\"F1 Macro\", color=\"#5cb85c\", alpha=0.85)\nb2 = ax.bar(x + width/2, acc_vals, width, label=\"Accuracy\", color=\"#5bc0de\", alpha=0.85)\nax.set_ylabel(\"Score\")\nax.set_title(\"Comparacion de Enfoques de Balanceo\")\nax.set_xticks(x); ax.set_xticklabels(enfoques); ax.legend()\nax.set_ylim(0, 1.0); ax.yaxis.grid(True, alpha=0.4)\nfor bar in b1: ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.01, f\"{bar.get_height():.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\nfor bar in b2: ax.text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.01, f\"{bar.get_height():.4f}\", ha=\"center\", va=\"bottom\", fontsize=9)\nplt.tight_layout()\nplt.savefig(\"../reports/comparacion_balanceo.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\nresumen = pd.DataFrame({\"Enfoque\": [\"Baseline\", \"Agrup.+ROS\", \"Agrup.+SMOTE\"],\n                         \"F1 Macro\": [f\"{v:.4f}\" for v in f1_vals],\n                         \"Accuracy\": [f\"{v:.4f}\" for v in acc_vals]})\nprint(resumen.to_string(index=False))\nidx = f1_vals.index(max(f1_vals))\nprint(f\"\\nMejor F1 Macro: {resumen.iloc[idx]['Enfoque']} ({max(f1_vals):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizaciones del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport seaborn as sns\n\nclases_best = sorted(y.unique())\ncm = confusion_matrix(y_test, y_pred_best, labels=clases_best)\n\nfig, ax = plt.subplots(figsize=(12, 9))\nConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clases_best).plot(\n    ax=ax, colorbar=True, cmap=\"Blues\", xticks_rotation=45\n)\nax.set_title(\"Matriz de Confusion - LinearSVC Optimizado\", fontsize=13)\nplt.tight_layout()\nplt.savefig(\"../reports/confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Guardado del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\nfrom sklearn.metrics import accuracy_score\n\nos.makedirs(\"../models\", exist_ok=True)\njoblib.dump(mejor_modelo,  \"../models/best_model.pkl\")\njoblib.dump(vectorizer_ag, \"../models/tfidf_vectorizer_final.pkl\")\n\nmetricas = {\n    \"modelo\": \"LinearSVC optimizado\",\n    \"mejor_C\": grid_search.best_params_,\n    \"accuracy\":    round(accuracy_score(y_test, y_pred_best), 4),\n    \"f1_macro\":    round(f1_score(y_test, y_pred_best, average=\"macro\", zero_division=0), 4),\n    \"f1_weighted\": round(f1_score(y_test, y_pred_best, average=\"weighted\", zero_division=0), 4),\n    \"f1_ros\":   round(f1_ros, 4),\n    \"f1_smote\": round(f1_smote, 4),\n}\nwith open(\"../reports/metrics.json\", \"w\") as f:\n    json.dump(metricas, f, indent=2)\n\nprint(\"Modelo guardado en ../models/best_model.pkl\")\nprint(\"Metricas guardadas en ../reports/metrics.json\")\nprint(json.dumps(metricas, indent=2))\n"
   ]
  }
 ]
}