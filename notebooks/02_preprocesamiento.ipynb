{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n\nhtml = \"\"\"\n<div style=\"background:linear-gradient(135deg,#1a1a2e,#16213e,#0f3460);border-radius:16px;padding:40px;text-align:center;font-family:'Segoe UI',Arial,sans-serif;box-shadow:0 8px 32px rgba(0,0,0,0.4);\">\n  <div style=\"font-size:2.5em;\">⚙️</div>\n  <h1 style=\"color:#e0e0e0;font-size:2em;margin:8px 0;\">Preprocesamiento de Texto</h1>\n  <p style=\"color:#a0aec0;letter-spacing:2px;text-transform:uppercase;font-size:0.95em;\">Limpieza · Lematizacion · TF-IDF · Vectorizacion</p>\n  <div style=\"background:rgba(255,255,255,0.06);border:1px solid rgba(255,255,255,0.1);border-radius:10px;padding:16px 24px;max-width:650px;margin:24px auto 0;text-align:left;color:#cbd5e0;font-size:0.93em;line-height:1.7;\">\n    Limpiamos los textos legales, aplicamos lematizacion y los transformamos en\n    vectores numericos con TF-IDF para que los modelos puedan procesarlos.\n    Guardamos los artefactos para ser reutilizados en el notebook de modelado.\n  </div>\n</div>\n\"\"\"\ndisplay(HTML(html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\ndf = pd.read_csv(\"../data/raw/legal_text_classification.csv\")\ndf = df.dropna(subset=[\"case_text\"]).reset_index(drop=True)\nprint(f\"Documentos cargados: {len(df):,}\")\ndf.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza con Lematizacion\n\nConvertimos a minusculas, eliminamos puntuacion y numeros, quitamos stopwords y aplicamos lematizacion para reducir palabras a su forma base (\"exercised\" → \"exercise\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n    tokens = text.split()\n    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n    return \" \".join(tokens)\n\ndf[\"clean_text\"] = df[\"case_text\"].astype(str).apply(clean_text)\ndf[[\"case_text\", \"clean_text\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Palabras mas Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\nimport matplotlib.pyplot as plt\n\nall_words = \" \".join(df[\"clean_text\"]).split()\nfreq = Counter(all_words)\ncommon = freq.most_common(20)\nwords, counts = zip(*common)\n\nplt.figure(figsize=(11, 5))\nplt.bar(words, counts, color=\"#4299e1\", alpha=0.85)\nplt.xticks(rotation=65)\nplt.title(\"Top 20 palabras mas frecuentes (texto limpio)\")\nplt.tight_layout()\nplt.savefig(\"../reports/prep_palabras_frecuentes.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorizacion TF-IDF\n\nUsamos bigramas para capturar frases como \"referred to\" o \"applied for\". `sublinear_tf=True` suaviza el peso de terminos muy frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\nimport joblib, os\n\nvectorizer = TfidfVectorizer(\n    max_features=10000,\n    ngram_range=(1, 2),\n    min_df=3,\n    max_df=0.85,\n    sublinear_tf=True\n)\n\nX = vectorizer.fit_transform(df[\"clean_text\"])\ny = df[\"case_outcome\"]\n\nprint(f\"Matriz TF-IDF: {X.shape}\")\nprint(f\"Documentos: {X.shape[0]:,}  |  Features: {X.shape[1]:,}\")\nprint(f\"\\nDistribucion de clases:\\n{y.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guardado de Artefactos\n\nGuardamos el vectorizador y el dataframe limpio para que el notebook de modelado los pueda reutilizar sin reprocesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os\nimport pickle\n\nos.makedirs(\"../models\", exist_ok=True)\n\n# Guardar vectorizador\njoblib.dump(vectorizer, \"../models/tfidf_vectorizer.pkl\")\n\n# Guardar matriz X, vector y y dataframe limpio\njoblib.dump(X, \"../models/X_tfidf.pkl\")\njoblib.dump(y, \"../models/y_labels.pkl\")\ndf[[\"case_text\", \"clean_text\", \"case_outcome\"]].to_csv(\"../data/raw/df_limpio.csv\", index=False)\n\nprint(\"Artefactos guardados en ../models/:\")\nfor f in os.listdir(\"../models\"):\n    size = os.path.getsize(f\"../models/{f}\") / 1024\n    print(f\"  {f}  ({size:.1f} KB)\")\n"
   ]
  }
 ]
}