{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/legal_text_classification.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui analizamos la estructura del dataset para entender que tenemos. Con info() revisamos si hay valores nulos y los tipos de datos. Con value_counts() observamos cuantos documentos hay por cada categoria. Esto es importante porque si una clase tiene muchos mas datos que otra, el modelo puede sesgarse. Esta etapa nos permite detectar problemas antes de comenzar el analisis profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()\n",
    "df['case_outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este bloque limpiamos los textos legales con un pipeline mejorado. Convertimos a minusculas, eliminamos numeros y puntuacion, quitamos stopwords y aplicamos **lematizacion** con WordNetLemmatizer: reduce palabras a su forma base (\"exercised\" -> \"exercise\", \"courts\" -> \"court\"), lo que reduce el vocabulario y mejora la generalizacion del modelo. Tambien eliminamos tokens de menos de 3 caracteres y las filas con texto vacio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df = df.dropna(subset=[\"case_text\"]).reset_index(drop=True)\n",
    "df[\"clean_text\"] = df[\"case_text\"].astype(str).apply(clean_text)\n",
    "df[[\"case_text\", \"clean_text\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui unimos todos los textos limpios y contamos cuantas veces aparece cada palabra. Esto permite identificar los terminos mas comunes en el lenguaje juridico del dataset. El grafico de barras muestra que palabras dominan el corpus y ayuda a detectar patrones tematicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_words = \" \".join(df[\"clean_text\"]).split()\n",
    "freq = Counter(all_words)\n",
    "common = freq.most_common(20)\n",
    "words, counts = zip(*common)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=70)\n",
    "plt.title(\"Top 20 palabras mas frecuentes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos el texto en vectores numericos usando **TF-IDF mejorado**. Ademas de palabras individuales (unigramas), incluimos **bigramas** (pares de palabras consecutivas), lo que captura frases como \"referred to\" o \"applied for\". Con min_df=3 eliminamos terminos rarisimos (ruido) y con max_df=0.85 descartamos los demasiado comunes. sublinear_tf=True aplica logaritmo al TF para suavizar pesos extremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "y = df[\"case_outcome\"]\n",
    "\n",
    "print(f\"Dimensiones de la matriz TF-IDF: {X.shape}\")\n",
    "print(f\"Distribucion de clases:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos el dataset con stratify=y para garantizar que cada particion tenga la misma proporcion de clases que el dataset original, lo cual es critico con datos desbalanceados. Luego entrenamos una Regresion Logistica con class_weight=\"balanced\", que ajusta automaticamente los pesos de cada clase inversamente proporcional a su frecuencia, dandole mas importancia a las clases minoritarias durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model_lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\", C=1.0)\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "print(\"=== Regresion Logistica (mejorada) ===\")\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparacion de Modelos\n\nEvaluamos cuatro algoritmos distintos para identificar cual ofrece mejor rendimiento. Usamos macro F1-score como metrica principal porque trata todas las clases por igual, penalizando el mal desempeno en clases minoritarias, algo critico dado el desbalance del dataset. Los modelos evaluados son: Regresion Logistica (ya entrenada), Naive Bayes Multinomial, LinearSVC (SVM lineal) y Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "modelos = {\n",
    "    \"Logistic Regression\": model_lr,\n",
    "    \"Naive Bayes\": MultinomialNB(alpha=0.1),\n",
    "    \"LinearSVC\": LinearSVC(class_weight=\"balanced\", max_iter=2000, C=1.0),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "resultados = {}\n",
    "for nombre, modelo in modelos.items():\n",
    "    if nombre != \"Logistic Regression\":\n",
    "        modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    acc = np.mean(y_pred == y_test)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    resultados[nombre] = {\"Accuracy\": acc, \"F1 Macro\": f1_macro, \"F1 Weighted\": f1_weighted}\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"  {nombre}\")\n",
    "    print(f\"  Accuracy:    {acc:.4f}\")\n",
    "    print(f\"  F1 Macro:    {f1_macro:.4f}\")\n",
    "    print(f\"  F1 Weighted: {f1_weighted:.4f}\")\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados).T.sort_values(\"F1 Macro\", ascending=False)\n",
    "print(\"\\n=== Ranking de modelos por F1 Macro ===\")\n",
    "print(df_resultados.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizacion de Hiperparametros con GridSearchCV\n\nAjustamos los hiperparametros del mejor modelo mediante busqueda en grilla con validacion cruzada de 5 pliegues. GridSearchCV prueba sistematicamente todas las combinaciones de parametros y selecciona la que maximiza el F1 macro promedio en validacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_svc = {\"C\": [0.01, 0.1, 1.0, 10.0]}\n",
    "svc_base = LinearSVC(class_weight=\"balanced\", max_iter=3000)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    svc_base, param_grid_svc,\n",
    "    cv=5, scoring=\"f1_macro\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejor parametro: {grid_search.best_params_}\")\n",
    "print(f\"Mejor F1 Macro en CV: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "mejor_modelo = grid_search.best_estimator_\n",
    "y_pred_best = mejor_modelo.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Mejor Modelo (LinearSVC optimizado) ===\")\n",
    "print(classification_report(y_test, y_pred_best, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaciones de Evaluacion\n\nLa **matriz de confusion** muestra cuantos documentos de cada clase real fueron clasificados en cada clase predicha. La diagonal principal son las predicciones correctas; los valores fuera de ella son errores. Esto revela que categorias se confunden entre si.\n\nLa **comparacion de F1 por clase** permite ver el avance respecto al modelo base original e identificar en cuales categorias el modelo mejorado tiene mayor ganancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "clases = sorted(y.unique())\n",
    "\n",
    "# --- Matriz de Confusion ---\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=clases)\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clases)\n",
    "disp.plot(ax=ax, colorbar=True, cmap=\"Blues\", xticks_rotation=45)\n",
    "ax.set_title(\"Matriz de Confusion - Mejor Modelo\", fontsize=14, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/confusion_matrix.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- F1 por clase: base vs mejor ---\n",
    "model_base = LogisticRegression(max_iter=1000)\n",
    "model_base.fit(X_train, y_train)\n",
    "y_pred_base = model_base.predict(X_test)\n",
    "\n",
    "f1_base = f1_score(y_test, y_pred_base, labels=clases, average=None, zero_division=0)\n",
    "f1_best_arr = f1_score(y_test, y_pred_best, labels=clases, average=None, zero_division=0)\n",
    "\n",
    "x = np.arange(len(clases))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.bar(x - width/2, f1_base, width, label=\"LR Base (original)\", color=\"#d9534f\", alpha=0.8)\n",
    "ax.bar(x + width/2, f1_best_arr, width, label=\"Mejor Modelo (optimizado)\", color=\"#5cb85c\", alpha=0.8)\n",
    "ax.set_xlabel(\"Clase\")\n",
    "ax.set_ylabel(\"F1-Score\")\n",
    "ax.set_title(\"Comparacion F1-Score por Clase: Modelo Base vs Mejor Modelo\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(clases, rotation=30, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.yaxis.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/f1_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# --- Ranking de modelos ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df_res = df_resultados.sort_values(\"F1 Macro\")\n",
    "ax.barh(df_res.index, df_res[\"F1 Macro\"], color=\"#5bc0de\", alpha=0.85)\n",
    "ax.set_xlabel(\"F1 Macro\")\n",
    "ax.set_title(\"Comparacion de Modelos - F1 Macro\")\n",
    "ax.xaxis.grid(True, alpha=0.4)\n",
    "for i, v in enumerate(df_res[\"F1 Macro\"]):\n",
    "    ax.text(v + 0.005, i, f\"{v:.4f}\", va=\"center\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del Mejor Modelo\n\nSerializamos el vectorizador TF-IDF y el modelo final con **joblib** para reutilizarlos sin necesidad de reentrenar. Esto es una practica estandar en produccion. Tambien guardamos un resumen de metricas en reports/metrics.json para registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(vectorizer, \"../models/tfidf_vectorizer.pkl\")\n",
    "joblib.dump(mejor_modelo, \"../models/best_model.pkl\")\n",
    "print(\"Modelo y vectorizador guardados en ../models/\")\n",
    "\n",
    "metricas = {\n",
    "    \"modelo\": \"LinearSVC (GridSearchCV optimizado)\",\n",
    "    \"mejores_parametros\": grid_search.best_params_,\n",
    "    \"accuracy\": round(accuracy_score(y_test, y_pred_best), 4),\n",
    "    \"f1_macro\": round(f1_score(y_test, y_pred_best, average=\"macro\", zero_division=0), 4),\n",
    "    \"f1_weighted\": round(f1_score(y_test, y_pred_best, average=\"weighted\", zero_division=0), 4),\n",
    "    \"comparacion_modelos\": df_resultados.round(4).to_dict(orient=\"index\")\n",
    "}\n",
    "\n",
    "with open(\"../reports/metrics.json\", \"w\") as f:\n",
    "    json.dump(metricas, f, indent=2)\n",
    "\n",
    "print(\"Metricas guardadas en ../reports/metrics.json\")\n",
    "print(f\"\\nResumen final:\")\n",
    "print(f\"  Accuracy:    {metricas['accuracy']}\")\n",
    "print(f\"  F1 Macro:    {metricas['f1_macro']}\")\n",
    "print(f\"  F1 Weighted: {metricas['f1_weighted']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte analizamos la polaridad y subjetividad del texto usando TextBlob. La polaridad mide si el lenguaje es positivo o negativo (rango -1 a 1), mientras que la subjetividad indica que tan objetivo o subjetivo es el texto (0 a 1). Aunque el lenguaje legal suele ser objetivo y neutral, este analisis permite detectar variaciones semanticas entre distintas categorias de casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text):\n",
    "        return 0.0, 0.0\n",
    "    blob = TextBlob(str(text))\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "df[\"polarity\"], df[\"subjectivity\"] = zip(*df[\"case_text\"].apply(get_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui graficamos la distribucion de polaridad y subjetividad para observar como se comportan los textos en general. Tambien calculamos el promedio de estas metricas por categoria para comparar si ciertos tipos de decisiones legales presentan lenguaje mas negativo, mas objetivo o mas subjetivo que otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(df[\"polarity\"], kde=True)\n",
    "plt.title(\"Distribucion de Polaridad\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(df[\"subjectivity\"], kde=True)\n",
    "plt.title(\"Distribucion de Subjetividad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"case_outcome\")[[\"polarity\", \"subjectivity\"]].mean()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}